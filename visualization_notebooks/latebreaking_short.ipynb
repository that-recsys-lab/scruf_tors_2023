{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import seaborn as sb\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BORDA_BASE = \"../new_data_gen/Data/history_file_synthetic_baseline_borda.json\"\n",
    "BORDA_LOT = \"../new_data_gen/Data/history_file_synthetic_product_lottery_borda.json\"\n",
    "BORDA_WEIGHT = \"../new_data_gen/Data/history_file_synthetic_weighted_alloc_borda.json\"\n",
    "BORDA_LEAST = \"../new_data_gen/Data/history_file_synthetic_least_fair_borda.json\"\n",
    "\n",
    "COPELAND_BASE = \"../new_data_gen/Data/history_file_synthetic_baseline_copeland.json\"\n",
    "COPELAND_LOT = \"../new_data_gen/Data/history_file_synthetic_product_lottery_copeland.json\"\n",
    "COPELAND_WEIGHT = \"../new_data_gen/Data/history_file_synthetic_weighted_alloc_copeland.json\"\n",
    "COPELAND_LEAST = \"../new_data_gen/Data/history_file_synthetic_least_fair_copeland.json\"\n",
    "\n",
    "RP_BASE = \"../new_data_gen/Data/history_file_synthetic_baseline_rankedpairs.json\"\n",
    "RP_LOT = \"../new_data_gen/Data/history_file_synthetic_product_lottery_rankedpairs.json\"\n",
    "RP_WEIGHT = \"../new_data_gen/Data/history_file_synthetic_weighted_alloc_rankedpairs.json\"\n",
    "RP_LEAST = \"../new_data_gen/Data/history_file_synthetic_least_fair_rankedpairs.json\"\n",
    "\n",
    "WR_BASE = \"../new_data_gen/Data/history_file_synthetic_baseline_weighted_rescore.json\"\n",
    "WR_LEAST = \"../new_data_gen/Data/history_file_synthetic_least_fair_weighted_rescore.json\"\n",
    "WR_LOT = \"../new_data_gen/Data/history_file_synthetic_product_lottery_weighted_rescore.json\"\n",
    "WR_WEIGHT = \"../new_data_gen/Data/history_file_synthetic_weighted_alloc_weighted_rescore.json\"\n",
    "\n",
    "recs_file = \"../new_data_gen/Data/recs.csv\"\n",
    "items_file = \"../new_data_gen/Data/items.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads recommender and item files\n",
    "recommender = pd.read_csv(recs_file, names=[\"User\",\"Item\",\"Score\"])\n",
    "item_features = pd.read_csv(items_file, names=[\"Item\",\"Feature\",\"BV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no need to change unless adding a new allocation mechanism\n",
    "history_files = [BORDA_BASE, BORDA_LOT, BORDA_WEIGHT, BORDA_LEAST, COPELAND_BASE, COPELAND_LEAST,COPELAND_LOT,COPELAND_WEIGHT,RP_BASE, RP_LEAST,RP_LOT,RP_WEIGHT, WR_BASE, WR_LEAST, WR_LOT, WR_WEIGHT]\n",
    "\n",
    "borda_b =[]\n",
    "borda_lo = []\n",
    "borda_w= []\n",
    "borda_le = []\n",
    "copeland_b =[]\n",
    "copeland_le= []\n",
    "copeland_lo = []\n",
    "copeland_w= []\n",
    "rp_b = []\n",
    "rp_le = []\n",
    "rp_lo= []\n",
    "rp_w= []\n",
    "wr_b = []\n",
    "wr_le = []\n",
    "wr_lo = []\n",
    "wr_w = []\n",
    "\n",
    "\n",
    "\n",
    "list_names = [borda_b, borda_lo, borda_w,borda_le,copeland_b, copeland_le,copeland_lo,copeland_w,rp_b,rp_le,rp_lo,rp_w,wr_b,wr_le,wr_lo,wr_w]\n",
    "readable_names = ['Borda Baseline','Borda Lottery', 'Borda Weighted Product', 'Borda Least Fair', 'Copeland Baseline','Copeland Least Fair', 'Copeland Lottery', 'Copeland Weighted Product','Ranked Pairs Baseline','Ranked Pairs Least Fair', 'Ranked Pairs Lottery', 'Ranked Pairs Weighted Product', 'Weighted Rescore Baseline', 'Weighted Rescore Lottery', 'Weighted Rescore Product', 'Weighted Rescore Least Fair']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for statistical analysis, change if adding new allocation mechanisms\n",
    "\n",
    "results_borda_b = {}\n",
    "results_borda_lo = {}\n",
    "results_borda_w= {}\n",
    "results_borda_le = {}\n",
    "results_copeland_b = {}\n",
    "results_copeland_le= {}\n",
    "results_copeland_lo = {}\n",
    "results_copeland_w= {}\n",
    "results_rp_b = {}\n",
    "results_rp_le = {}\n",
    "results_rp_lo = {}\n",
    "results_rp_w = {}\n",
    "results_wr_b = {}\n",
    "results_wr_le = {}\n",
    "results_wr_lo = {}\n",
    "results_wr_w = {}\n",
    "\n",
    "\n",
    "dict_names = [results_borda_b,results_borda_lo, results_borda_w, results_borda_le, results_copeland_b,results_copeland_le, results_copeland_lo, results_copeland_w, results_rp_b,results_rp_le, results_rp_lo, results_rp_w, results_wr_b, results_wr_le, results_wr_lo, results_wr_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read history files\n",
    "for history_file, list_name in zip(history_files,list_names):\n",
    "    with jsonlines.open(history_file) as reader:\n",
    "        for obj in reader:\n",
    "            list_name.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process history files into dictionaries for statistics\n",
    "for list, dictionary in zip(list_names, dict_names):\n",
    "    for line in list:\n",
    "        results = line['choice_out']['results']\n",
    "        results_list = []\n",
    "        for item in results:\n",
    "            results_list.append(item['item'])\n",
    "        dictionary[line['user']] = results_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATISTICS FUNCTIONS ###\n",
    "\n",
    "# get recommendation score based on user and item ids\n",
    "def lookupscore(user, item):\n",
    "    user = int(user)\n",
    "    item = int(item)\n",
    "    score = recommender.loc[(recommender.User == user) & (recommender.Item == item)][\"Score\"]\n",
    "    return float(score)\n",
    "\n",
    "# calculate ndcg given a list of recommended and ideal scores\n",
    "def ndcg(scores1, scores2):\n",
    "    idealdcg = 0.0\n",
    "    recdcg = 0.0\n",
    "    for index, val in enumerate(scores1):\n",
    "        recdcg += (2**val - 1)/np.log2(index + 2)\n",
    "    for index, val in enumerate(scores2):\n",
    "         idealdcg += (2**val - 1)/np.log2(index + 2)\n",
    "    return recdcg/idealdcg\n",
    "\n",
    "# not currently run in this file\n",
    "def plot_ndcg(name,ndcg_results):\n",
    "    ndcg_result = ndcg_results[name]\n",
    "    ndcg_data = pd.DataFrame(ndcg_result)\n",
    "    sb.lineplot(ndcg_data)\n",
    "    imagefile = name + \"ndcgplot.png\"\n",
    "    plt.savefig(imagefile)\n",
    "\n",
    "# given an item id return a list of its features as binary values\n",
    "def get_item_features(item_id):\n",
    "    feature_values = []\n",
    "    for value in item_features.loc[(item_features.Item == int(item_id))][\"BV\"]:\n",
    "        feature_values.append(value)\n",
    "    return feature_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VISUALIZATION FUNCTIONS ###\n",
    "# TODO: Update viz functions to make them more efficient and inline with statistics\n",
    "\n",
    "def process_history(history, fair=True, compat=True, alloc=True, lists=True):\n",
    "    if fair:\n",
    "        fair_list = [entry['allocation']['fairness scores'] for entry in history]\n",
    "        fair_df = pd.DataFrame(fair_list)\n",
    "    else:\n",
    "        fair_df = None\n",
    "    if compat:\n",
    "        compat_list = [entry['allocation']['compatibility scores'] for entry in history]\n",
    "        compat_df = pd.DataFrame(compat_list)\n",
    "    else:\n",
    "        compat_df = None\n",
    "    if alloc:\n",
    "        alloc_list = [entry['allocation']['output'] for entry in history]\n",
    "        alloc_df = pd.DataFrame(alloc_list)\n",
    "        alloc_df['none'] = (alloc_df['1'] == 0) & (alloc_df['2'] == 0)\n",
    "    else:\n",
    "        alloc_df = None\n",
    "    if lists:\n",
    "        results_list = [process_results(entry['choice_out']['results']) for entry in history]\n",
    "    else:\n",
    "        results_list = None\n",
    "    return fair_df, compat_df, alloc_df, results_list\n",
    "\n",
    "def process_results(result_structs):\n",
    "    return [(entry['item'], entry['score']) for entry in result_structs]\n",
    "\n",
    "def plot_fairness_time(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness\")\n",
    "    sb.lineplot(data=fair_df)\n",
    "    image_file = image_prefix + '-fairness.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_allocation(experiment_data, include_none=False, image_prefix=None):\n",
    "    alloc_df = pd.DataFrame(experiment_data[2])\n",
    "    if include_none is False:\n",
    "        if not alloc_df['none'][1:].any():\n",
    "            alloc_df.drop('none', axis=1, inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Allocation\")\n",
    "    sb.lineplot(data=alloc_df.cumsum())\n",
    "    image_file = image_prefix + '-allocation.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_fairness_regret(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    regret = 1-fair_df\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness Regret\")\n",
    "    sb.lineplot(data=regret.cumsum())\n",
    "    image_file = image_prefix + '-regret.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def do_plots(experiment_data, include_none=False, image_prefix=None):\n",
    "    plot_fairness_time(experiment_data, include_none, image_prefix)\n",
    "    plot_allocation(experiment_data, include_none, image_prefix)\n",
    "    plot_fairness_regret(experiment_data, include_none, image_prefix)\n",
    "\n",
    "def process(experiment, include_none=False, image_prefix=None):\n",
    "    experiment_data = process_history(experiment)\n",
    "    do_plots(experiment_data, include_none, image_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates average NDCG for each allocation mechanism and stores them in the list avg_of_ndcg\n",
    "# also creates ndcg_results, a dictionary that allows plotting NDCG over time for each mechanism\n",
    "avg_of_ndcg = []\n",
    "ndcg_results = {}\n",
    "for dictionary, name in zip(dict_names, readable_names):\n",
    "    users = []\n",
    "    ndcg_values = []\n",
    "    for user, items in dictionary.items():\n",
    "        scores = []\n",
    "        for item in items:\n",
    "            scores.append(lookupscore(user, item))\n",
    "        ideal_scores = []\n",
    "        for score in recommender.loc[(recommender.User == int(user))][\"Score\"].sort_values(ascending=False):\n",
    "            ideal_scores.append(score)\n",
    "            ideal_scores = ideal_scores[0:len(scores)+1]\n",
    "        ndcg_values.append(ndcg(scores, ideal_scores))\n",
    "        users.append(user)\n",
    "    avg_of_ndcg.append(mean(ndcg_values))\n",
    "    ndcg_results[name] = {\"Users\":users, \"NDCG\":ndcg_values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates dataframe of average NDCG values\n",
    "ndcg_table = pd.DataFrame(data=avg_of_ndcg, index=readable_names, columns=[\"NDCG\"])\n",
    "ndcg_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates proportional fairness for the representation of item features\n",
    "# currently designed to handle two three features\n",
    "# TODO: make it easier to change number of item features\n",
    "either_fairness = []\n",
    "f1_representation = []\n",
    "f2_representation = []\n",
    "\n",
    "for dictionary, name in zip(dict_names, readable_names):\n",
    "    item_counter = 0\n",
    "    f0 = 0\n",
    "    f1 = 0\n",
    "    f2 = 0\n",
    "    either = 0\n",
    "    for items in dictionary.values():\n",
    "        for item in items:\n",
    "            item_counter += 1\n",
    "            list_of_features = get_item_features(item)\n",
    "            if list_of_features[0] == 1:\n",
    "                f0 += 1\n",
    "            if list_of_features[1] == 1:\n",
    "                f1 += 1\n",
    "            if list_of_features[2] == 1:\n",
    "                f2 += 1\n",
    "            if list_of_features[1] == 1 or list_of_features[2] == 1:\n",
    "                either += 1\n",
    "    either_fairness.append(either/item_counter)\n",
    "    f1_representation.append(f1/item_counter)\n",
    "    f2_representation.append(f2/item_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculates average fairness in relation to threshold\n",
    "agent1 = ['1']\n",
    "prop1 = .25\n",
    "agent2 = ['2']\n",
    "prop2 = .25\n",
    "fairness_results = {\"Protected\":either_fairness,\"1\":f1_representation,\"2\":f2_representation}\n",
    "prop_fairness_results = pd.DataFrame(data=fairness_results, index=readable_names)\n",
    "prop_fairness_results['score1'] = prop_fairness_results[agent1]/prop1\n",
    "prop_fairness_results['score2'] = prop_fairness_results[agent2]/prop2\n",
    "prop_fairness_results['Average Fairness Score'] = (prop_fairness_results['score1']+prop_fairness_results['score2'])/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness = prop_fairness_results.merge(ndcg_table, left_index = True, right_index = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "baseline_accuracy = ndcg_fairness.iloc[0,5]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness.loc[0:4, \"Choice\"] = \"Borda\"\n",
    "ndcg_fairness.loc[4:8, \"Choice\"] = \"Copeland\"\n",
    "ndcg_fairness.loc[8:12, \"Choice\"] = \"Ranked Pairs\"\n",
    "ndcg_fairness.loc[12:16, \"Choice\"] = \"Rescoring\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#ndcg_fairness[\"Allocation\"] = ndcg_fairness.apply(lambda x: x.name.replace(x[\"Choice\"], ''), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness = ndcg_fairness.drop([\"Borda Baseline\", \"Copeland Baseline\", \"Ranked Pairs Baseline\", \"Weighted Rescore Baseline\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness[\"Allocation\"] = ''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness.iloc[0,8] = \"Lottery\"\n",
    "ndcg_fairness.iloc[1,8] = \"Weighted\"\n",
    "ndcg_fairness.iloc[2,8] = \"Least Fair\"\n",
    "ndcg_fairness.iloc[3:10:3,8] = \"Least Fair\"\n",
    "ndcg_fairness.iloc[4:11:3,8] = \"Lottery\"\n",
    "ndcg_fairness.iloc[5:12:3,8] = \"Weighted\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness[\"Proportional Difference\"] = ndcg_fairness.apply(lambda x: abs(x[\"1\"] - x[\"2\"]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ndcg_fairness[\"Normalized Difference\"] = ndcg_fairness.apply(lambda x: abs(x[\"score1\"] - x[\"score2\"]), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "difference_table = ndcg_fairness[[\"Proportional Difference\", \"Normalized Difference\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "difference_table = difference_table[0:16]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sb.set_style(\"white\")\n",
    "plot = sb.scatterplot(x='Average Fairness Score',\n",
    "                      y='NDCG',\n",
    "                      data=ndcg_fairness,\n",
    "                      style=\"Allocation\",\n",
    "                      hue=\"Choice\",\n",
    "                      s=80)\n",
    "\n",
    "sb.set(font_scale = 1.0)\n",
    "#plt.axhline(y = baseline_accuracy, linestyle=\"dashed\")\n",
    "plt.xlabel(\"Average Fairness Score\", fontsize=11, labelpad=10)\n",
    "plt.ylabel(\"nDCG\", fontsize=10, labelpad=11)\n",
    "plt.legend(loc=\"lower left\", fontsize=10.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
