{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import gca\n",
    "from matplotlib import figure\n",
    "import seaborn as sb\n",
    "#pip install numba==0.57.0\n",
    "import numba as nb\n",
    "from numba import njit\n",
    "from numba.core import types\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate array for denom of ndcg calcs\n",
    "rec_list = 10\n",
    "base_logs = np.log2(np.arange(rec_list)+2)\n",
    "\n",
    "@njit\n",
    "def calculate_ndcg(histories: types.DictType(types.unicode_type, types.DictType(types.unicode_type, types.float64[:])), \n",
    "                  recommender: types.DictType(types.int64, types.UniTuple(types.float64[:], 2)),\n",
    "                  base_logs: types.float64[:]) -> types.float64[:]:\n",
    "    avg_of_ndcg = []\n",
    "    for history in histories:\n",
    "        i_count = 0\n",
    "        all_ndcg = 0\n",
    "        for user, items in histories[history].items():\n",
    "            scores = np.empty(len(items), dtype=np.float64)\n",
    "            #scores = []\n",
    "            for item in items:\n",
    "                idx_array = np.asarray(recommender[user][1] == item).nonzero()[0]\n",
    "                if idx_array.size != 0:\n",
    "                    idx = idx_array[0]\n",
    "                    score = recommender[user][0][idx]\n",
    "                else:\n",
    "                    score = 0.0\n",
    "                scores.append(score)\n",
    "            #scores = np.asarray(scores)\n",
    "            ideal_scores = np.sort(recommender[user][0])[::-1][:len(scores)]\n",
    "            scores[scores > 0] = 1.0\n",
    "            ideal_scores[ideal_scores > 0] = 1.0\n",
    "            recdcg = np.sum(scores/base_logs)\n",
    "            idealdcg = np.sum(ideal_scores/base_logs)\n",
    "            if idealdcg == 0.0:\n",
    "                ndcg = 0.0\n",
    "            else:\n",
    "                ndcg = recdcg/idealdcg\n",
    "            i_count += 1\n",
    "            all_ndcg += ndcg\n",
    "\n",
    "        avg_of_ndcg.append(all_ndcg/i_count)\n",
    "    return avg_of_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATISTICS FUNCTIONS ###\n",
    "\n",
    "# implicit type detection for pandas lookups\n",
    "def typecast(series, var):\n",
    "    dtype = pd.api.types.infer_dtype(series)\n",
    "    dtypes = {\"string\":str,\"integer\":int,\"floating\":float,\"mixed-integer-float\":float}\n",
    "    if type(var) != dtypes[dtype]:\n",
    "        var = dtypes[dtype](var)\n",
    "    if dtype not in dtypes.keys():\n",
    "        warnings.warn(\"Type of column \"+series.name+\" could not be implicitly determined. Defaulting to integer...\")\n",
    "        var = int(var)\n",
    "    return var\n",
    "\n",
    "# given an item id return a list of its features as binary values\n",
    "def get_item_features(item_features, item_id):\n",
    "    country = item_features.loc[(item_features.Item == typecast(item_features.Item, item_id)) & (item_features.Feature == typecast(item_features.Feature, \"COUNTRY_low_pfr\"))][\"BV\"]\n",
    "    loan_size = item_features.loc[(item_features.Item == typecast(item_features.Item, item_id)) & (item_features.Feature == typecast(item_features.Feature, \"loan_buck_5\"))][\"BV\"]\n",
    "    return (country, loan_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VISUALIZATION FUNCTIONS ###\n",
    "\n",
    "def process_history(history, fair=True, compat=True, alloc=True, lists=True):\n",
    "    if fair:\n",
    "        fair_list = [entry['allocation']['fairness scores'] for entry in history]\n",
    "        fair_df = pd.DataFrame(fair_list)\n",
    "    else:\n",
    "        fair_df = None\n",
    "    if compat:\n",
    "        compat_list = [entry['allocation']['compatibility scores'] for entry in history]\n",
    "        compat_df = pd.DataFrame(compat_list)\n",
    "    else:\n",
    "        compat_df = None\n",
    "    if alloc:\n",
    "        alloc_list = [entry['allocation']['output'] for entry in history]\n",
    "        alloc_df = pd.DataFrame(alloc_list)\n",
    "        alloc_df['none'] = (alloc_df['COUNTRY_low_pfr'] == 0) & (alloc_df['loan_buck_5'] == 0)\n",
    "    else:\n",
    "        alloc_df = None\n",
    "    if lists:\n",
    "        results_list = [process_results(entry['choice_out']['results']) for entry in history]\n",
    "    else:\n",
    "        results_list = None\n",
    "    return fair_df, compat_df, alloc_df, results_list\n",
    "\n",
    "def process_results(result_structs):\n",
    "    return [(entry['item'], entry['score']) for entry in result_structs]\n",
    "\n",
    "def plot_fairness_time(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness\")\n",
    "    sb.lineplot(data=fair_df)\n",
    "    image_file = image_prefix + '-fairness.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_allocation(experiment_data, include_none=False, image_prefix=None):\n",
    "    alloc_df = pd.DataFrame(experiment_data[2])\n",
    "    if include_none is False:\n",
    "        if not alloc_df['none'][1:].any():\n",
    "            alloc_df.drop('none', axis=1, inplace=True)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Allocation\")\n",
    "    sb.lineplot(data=alloc_df.cumsum())\n",
    "    image_file = image_prefix + '-allocation.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def plot_fairness_regret(experiment_data, include_none=False, image_prefix=None):\n",
    "\n",
    "    fair_df = experiment_data[0]\n",
    "    regret = 1-fair_df\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sb.set(font_scale=2)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fairness Regret\")\n",
    "    sb.lineplot(data=regret.cumsum())\n",
    "    image_file = image_prefix + '-regret.png'\n",
    "    plt.savefig(image_file)\n",
    "\n",
    "def do_plots(experiment_data, include_none=False, image_prefix=None):\n",
    "    plot_fairness_time(experiment_data, include_none, image_prefix)\n",
    "    plot_allocation(experiment_data, include_none, image_prefix)\n",
    "    plot_fairness_regret(experiment_data, include_none, image_prefix)\n",
    "\n",
    "def process(experiment, include_none=False, image_prefix=None):\n",
    "    experiment_data = process_history(experiment)\n",
    "    do_plots(experiment_data, include_none, image_prefix)\n",
    "\n",
    "def process_names(name):\n",
    "    orig_name = name\n",
    "    for alloc in [\"Baseline\",\"Lottery\",\"Weighted Product\",\"Least Fair\"]:\n",
    "        name = name.replace(alloc, \"\")\n",
    "        if name != orig_name:\n",
    "            return name.rstrip(), alloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data/\" \n",
    "#Specify the base names for each experiment\n",
    "base_names = [\n",
    "    \"history_file_baseline\",\n",
    "    \"history_file_least\",\n",
    "    \"history_file_ofair\",\n",
    "    \"history_file_product\",\n",
    "    \"history_file_weighted\"\n",
    "\n",
    "]\n",
    "\n",
    "item_features = pd.read_cvs('items_s1.csv')\n",
    "recommender = pd.read_csv('recs_s1.csv', names=[\"User\",\"Item\",\"Score\"])\n",
    "\n",
    "# Generate file paths and mechanisms dynamically\n",
    "history_files = []\n",
    "mechanisms = []\n",
    "for base_name in base_names:\n",
    "    history_file_path = os.path.join(base_path, f\"{base_name}.json\")\n",
    "    mechanism_name = f\"{base_name}\"\n",
    "    history_files.append(history_file_path)\n",
    "    mechanisms.append(mechanism_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_histories = {}\n",
    "for mechanism in mechanisms:\n",
    "    processed_histories[mechanism] = {}\n",
    "    processed_histories[mechanism][\"History\"] = []\n",
    "for history_file, mechanism in zip(history_files,mechanisms):\n",
    "    with jsonlines.open(history_file) as reader:\n",
    "        for obj in reader:\n",
    "            processed_histories[mechanism][\"History\"].append(obj)\n",
    "for mechanism in mechanisms:\n",
    "    processed_histories[mechanism][\"Statistics\"] = {}\n",
    "for mechanism in mechanisms:\n",
    "    for line in processed_histories[mechanism][\"History\"]:\n",
    "        results = line['choice_out']['results']\n",
    "        results_list = []\n",
    "        for result in results:\n",
    "            results_list.append(result['item'])\n",
    "        processed_histories[mechanism][\"Statistics\"][line['user']] = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert history + recommender info to typed dicts for numba\n",
    "light_histories = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.DictType(types.unicode_type, types.float64[:]),\n",
    ")\n",
    "for history in processed_histories:\n",
    "    consolidated_statistics = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.float64[:]\n",
    "    )\n",
    "    for user, items in processed_histories[history][\"Statistics\"].items():\n",
    "        consolidated_statistics[user] = np.asarray(items, dtype='f8')\n",
    "    light_histories[history] = consolidated_statistics\n",
    "\n",
    "light_recommender = Dict.empty(\n",
    "    key_type=types.unicode_type,\n",
    "    value_type=types.UniTuple(types.float64[:], 2),\n",
    ")\n",
    "for user in recommender[\"User\"].unique():\n",
    "    scores = recommender[recommender[\"User\"] == user][\"Score\"].to_numpy(dtype='f8')\n",
    "    items = recommender[recommender[\"User\"] == user][\"Item\"].to_numpy(dtype='f8')\n",
    "    light_recommender[user] = (scores, items)\n",
    "avg_of_ndcg = calculate_ndcg(light_histories, light_recommender, base_logs)\n",
    "# creates dataframe of average NDCG values\n",
    "ndcg_table = pd.DataFrame(data=avg_of_ndcg, index=mechanisms, columns=[\"NDCG\"])\n",
    "ndcg_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates adj proportional fairness for the representation of item features\n",
    "num_features = 10\n",
    "feature_names = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "protected_features = [\"0\", \"1\"]\n",
    "fairness_targets = [0., 0.]\n",
    "\n",
    "proportional_fairness = []\n",
    "adj_fairness = []\n",
    "for history, name in zip(processed_histories, mechanisms):\n",
    "    item_counter = 0\n",
    "    features = [0]*num_features\n",
    "    for items in processed_histories[history][\"Statistics\"].values():\n",
    "        for item in items:\n",
    "            item_counter += 1\n",
    "            for idx, val in enumerate(get_item_features(item_features, item)):\n",
    "                features[idx] = features[idx] + val\n",
    "    proportional = [x/item_counter for x in features]\n",
    "    proportional_fairness.append(proportional)\n",
    "    i = 0\n",
    "    calc_adj_fairness = []\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        if name in protected_features:\n",
    "            fair_target = fairness_targets[i]\n",
    "            calc_adj_fairness.append(proportional[idx]/fair_target)\n",
    "            i = i+1\n",
    "    adj_fairness.append(calc_adj_fairness)\n",
    "prop_fairness_results = pd.DataFrame(data=adj_fairness, columns=protected_features, index=mechanisms)\n",
    "ndcg_fairness = prop_fairness_results.merge(ndcg_table, left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fairness data to create boxplots\n",
    "# create df w/ allocation/choice mechs and fairness scores over time\n",
    "experiment_data = []\n",
    "for history in processed_histories:\n",
    "    choice, alloc = process_names(history)\n",
    "    experiment = pd.DataFrame(process_history(processed_histories[history][\"History\"])[0])\n",
    "    experiment[\"Choice Mech\"] = choice\n",
    "    experiment[\"Allocation Mech\"] = alloc\n",
    "    experiment_data.append(experiment)\n",
    "experiments = pd.concat(experiment_data)\n",
    "experiments[\"Time\"] = experiments.index\n",
    "experiments = pd.melt(experiments, id_vars=['Allocation Mech',\"Choice Mech\",\"Time\"], value_vars=[\"0\",\"1\"],var_name='Agent', value_name=\"Fairness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store baseline means, then remove baseline\n",
    "baseline1_mean = experiments[experiments[\"Allocation Mech\"] == \"Baseline\"][experiments[\"Agent\"] == \"0\"][\"Fairness\"].mean()\n",
    "baseline2_mean = experiments[experiments[\"Allocation Mech\"] == \"Baseline\"][experiments[\"Agent\"] == \"1\"][\"Fairness\"].mean()\n",
    "boxplots = experiments[experiments[\"Allocation Mech\"] != \"Baseline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set(font_scale=1.5)\n",
    "\n",
    "g = sb.catplot(data=boxplots,\n",
    "           row=\"Allocation Mech\",\n",
    "           col=\"Choice Mech\",\n",
    "           x=\"Fairness\",\n",
    "           y=\"Agent\",\n",
    "           order=[\"0\",\"1\"],\n",
    "           kind=\"box\",\n",
    "           height=2,\n",
    "           aspect=2,\n",
    "           margin_titles=True)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.axvline(x=baseline1_mean, color='tab:blue', ls=\"dashed\", lw=3)\n",
    "    ax.axvline(x=baseline2_mean, color='tab:orange', ls=\"dashed\", lw=3)\n",
    "\n",
    "g.set_titles(row_template='{row_name}', col_template='{col_name}')\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scatterplot(fairness_df, base_ndcg, filename=None):\n",
    "    sb.set_style(\"white\")\n",
    "    plot = sb.scatterplot(x='Average Fairness Score',\n",
    "                      y='NDCG',\n",
    "                      data=fairness_df,\n",
    "                      style=\"Allocation\",\n",
    "                      hue=\"Choice\",\n",
    "                      s=100,\n",
    "                      markers={\"Lottery\": \"^\", \"Weighted\": \"X\", \"Least Fair\": \"o\"})\n",
    "\n",
    "    sb.set(font_scale = 1.25)\n",
    "    plt.axhline(y = base_ndcg, linestyle=\"dashed\")\n",
    "    plt.xlabel(\"Average Fairness Score\", fontsize=12, labelpad=10)\n",
    "    plt.ylabel(\"nDCG\", fontsize=12, labelpad=11)\n",
    "    plt.legend(loc=\"lower left\", fontsize=12) # location for plots in paper, may need to change with other data\n",
    "    plt.tight_layout()\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_scatterplot(ndcg_fairness, baseline_accuracy, filename='synthetic_scatter.png')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
